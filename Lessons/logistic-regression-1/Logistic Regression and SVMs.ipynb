{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "# Intro to Classification and Model Tuning\n",
    "### Logistic Regression - SVC - Baseline Accuracy - Hyperparameter Tuning\n",
    "\n",
    "---\n",
    "\n",
    "### Key Terms:\n",
    "- Logistic Regression \n",
    "- Support Vector Machine\n",
    "- Baseline Baseline Accuracy\n",
    "- Model Hyperparameter\n",
    "\n",
    "### Expected Knowledge:\n",
    "- Manipulating data with Pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning about two popular classification models\n",
    "\n",
    "### Logistic Regression \n",
    "\n",
    "![Linear against log](./assets/Lin_vs_log.png)\n",
    "\n",
    "[Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression)\n",
    "\n",
    "It's a simple supervised model that works relatively well to classify data against our training set.\n",
    "    \n",
    "**Pros**:\n",
    "    - Simple & works surprisingly well most of the time\n",
    "    - Fast to train\n",
    "    - Easy to interpret\n",
    "    - Gives confidence value for its predictsions\n",
    "    - Implicityly considers all linear combinations of features\n",
    "    - It is scale agnostic (0 ->1 )\n",
    "  \n",
    "**Cons**:\n",
    "    - Not Multilabel\n",
    "    - Somewhat susceptible to outliers\n",
    "    - Struggles with collinear features\n",
    "    - It's... simple\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expanding our understanding of Machine Learning by looking at multiple models together\n",
    "\n",
    "As we get further into class we'll be exploring additional models. Most, if not all of them, follow the 4 steps of sklearn model building quite closely\n",
    "\n",
    ">1. Import the model\n",
    ">2. Instantiate a model object\n",
    ">3. Fit it to data\n",
    ">4. Make a prediction\n",
    "\n",
    "There are a few nuances to tuning parameters and how a model is constructed but you should be able to begin independently exploring the documentation by this point as you look for options for your own capstone project. The first additional model we'll add is a Support Vector Machine or SVM. As we continue through class you'll be exposed to 20+ model types to give you a strong background is exploring the full world of machine learning after class.\n",
    "\n",
    "The SVM was the original king of machine learning as it used an insightful trick to be able to quickly make predictions after a model was fit. While new models have arisen since it's hayday 10 years ago, it's still a powerful and effective model. Let's learn a little bit about it.\n",
    "\n",
    "### Support Vector Machine Classifier\n",
    "\n",
    "\n",
    "- [Support Vector Machine](https://en.wikipedia.org/wiki/Support_vector_machine)\n",
    "\n",
    "**Background** The goal of a support vector machine is to find  the optimal separating hyperplane which maximizes the margin of the training data. \n",
    "\n",
    "**Pros**:\n",
    "    - Powerful Model\n",
    "    - Popularized modern machine learning due to it's extreme power (dethroned by Deep Learning)\n",
    "    - Robust to outliers\n",
    "    - Uses the kernel trick \n",
    "      \n",
    "**Cons**:\n",
    "    - Many possible settings\n",
    "    - Slow to train\n",
    "    - Scale matters\n",
    "    - Can be a black box (it's hard to understand how or why it makes predictions)\n",
    "    - Does not provide predicted probabilities\n",
    "    \n",
    "**What do we know?**\n",
    "\n",
    "- It needs training data so it's a supervised model\n",
    "- We're using it for classification\n",
    "\n",
    "**What do we not know**\n",
    "- How does it make predictions?\n",
    "- What is a hyperplane?\n",
    "\n",
    "Those questions are inherently linked. A **hyperplane** is the seperation of space between our classes. If we break it down further we can better understand it.\n",
    "\n",
    "- in one dimension, an hyperplane is called a point\n",
    "- in two dimensions, it is a line\n",
    "- in three dimensions, it is a plane\n",
    "- in more dimensions you can call it an hyperplane\n",
    "\n",
    "As we start to understand that concept it begs another question. I can draw alot of lines so which is the right one? That's the goal of the SVM - **Finding the optimal hyperplane**. Finding this optimal hyperplane is dependent on a few particular vectors that support its placement - or support vectors.\n",
    "\n",
    "Support Vectors are the data points closest to the hyperplane or decision line. These are the data points that are the **most difficult** to classify. Given their proximity to the hyperplane they have direct bearing on its optimum location. Essentially Support vectors would change the elements of the training set if moved or removed and are critical elements of the training set.\n",
    "\n",
    "![image.png](./assets/Support Vector.png)\n",
    "\n",
    "\n",
    "**Now to optimize our hyperlane!**\n",
    "\n",
    "Step 1 - Seperate the plane as far as you can from data\n",
    "![Optimal Hyperplane](./assets/optimal-hyperplane.png)\n",
    "\n",
    "Step 2 - Find the hyperplane with the largest margin\n",
    "\n",
    "For any hyperplane we can compute the margin.\n",
    " - Find the distance between the hyperplane and the closest data point. \n",
    " - Take that distance and double it\n",
    " \n",
    "Now you have the **margin**. Basically the margin is an area where you will not find any data points. (Note: this can cause some problems when data is noisy)\n",
    "\n",
    "![Margin](./assets/margin.png)\n",
    "\n",
    "\n",
    "\n",
    "### Kernel Trick: For when our data isn't already linearly separable\n",
    "\n",
    "The below picture shows the true magic behind a Support Vector Machine. The objects on the left are mapped as we'd originally find them. A full seperation would require a curve and thus more complexity than drawing a line. In a support vector machine we rearrange using a set of mathmatical functions known as **kernels**. An intuitive way to think about kernels is a similarity function. Given two objects the kernel outputs some similarity score. The simpliest example is the linear kernel or dot-product. Given two vectors, the similarity is the lenght of hte projections of one vector to another. Given a data point to classify, the decision function makes use of the kernel by comparing that data point to a number of support vectors weighted by the learned parameters. The support vectors are in the domain of that data point and along the learned parameters are found by the learning algorithm. \n",
    "\n",
    "![Kernels](./assets/Input_Feature.gif)\n",
    "\n",
    "### Sometimes it's helpful to see this in 3d\n",
    "\n",
    "While it's hard to draw a line to divide the classes in the picture on the left, it's easy on the right\n",
    "\n",
    "![Kernels-2d-to-3d](./assets/2d-to-3d.png)\n",
    "\n",
    "### Another example\n",
    "\n",
    "<img src=\"./assets/2d-to-3d-projection.jpeg\" width=\"600\">\n",
    "\n",
    "#### Parameters\n",
    "\n",
    ">class sklearn.svm.SVC(C=1.0, kernel=’rbf’, degree=3, gamma=’auto’, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=’ovr’, random_state=None)\n",
    "\n",
    "Today we're going to adjust C. The C parameter trades off misclassification of training examples against simplicity of the decision surface. A low C makes the decision surface smooth, while a high C aims at classifying all training examples correctly\n",
    "by giving the model freedom to select more samples as support vectors. \n",
    "\n",
    "![BigC_LittleC](./assets/BigC_LittleC.png)\n",
    "\n",
    "As we regularize, or penalize C, we can visualize the impact on our predictions\n",
    "![Regularize](./assets/c_regulation.png)\n",
    "\n",
    "Note: \n",
    "* Here's a good source on [understanding the math](https://www.svm-tutorial.com/2014/11/svm-understanding-math-part-1/) or [The Idiot's Guide to SVM](http://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf)\n",
    "* Learn more about [Kernels](https://en.wikipedia.org/wiki/Kernel_method#Mathematics:_the_kernel_trick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library Imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np \n",
    "%matplotlib inline \n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson Structure:\n",
    "\n",
    "In this lesson, you will build two types of classifiers\n",
    "\n",
    "1. A [Logistic Regression Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "2. A [Suport Vector Machine Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC)\n",
    "\n",
    "We will train these models on a dataset of tumors, aiming to classify a given tumor as benign or malignant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps:\n",
    "---\n",
    "**1. EDA**\n",
    "    - Analyze the dataset, mostly through visualizations.\n",
    "**2. Data Preparation**\n",
    "    - Handle non-numeric features.\n",
    "    - Handle missing values.\n",
    "    - Compute the baseline accuracy.\n",
    "**3. Model Training**\n",
    "    - Performing a train-test split on our data\n",
    "    - Fitting the models of your choice (today: LogisticRegression and SVC) on the training data.\n",
    "    - Scoring our trained models on the test data.\n",
    "    \n",
    "**4. Model Evaluation**\n",
    "    - Gaining an understanding of how our model makes predictions.\n",
    "    - Tuning our model to obtain better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://www.biosytechworld.com/media/3188/123.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/cancer_uci.csv', index_col='Unnamed: 0')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifying Data Integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of rows and columns of our dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data types and verify we have no missing values\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming dtypes\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For our non-numeric columns, explore how many (and what) categories we have\n",
    "df['Bare_Nuclei'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking for pairwise correlations between the features (Heatmap)\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: As a learning experience, run sns.pairplot on this data\n",
    "# Also try: sns.pairplot(df, hue='Class', plot_kws={'alpha':0.1})\n",
    "sns.heatmap(df.corr(),annot=True,linewidths=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby is also a great EDA tool\n",
    "\n",
    "df.groupby('Class').mean().plot(kind='bar', figsize=(20,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns you will not use\n",
    "data = df.drop(['Sample_code_number'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle 'bad data'\n",
    "data['Bare_Nuclei'] = data['Bare_Nuclei'].apply(lambda x: np.nan if x =='?' else float(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing a python library\n",
    "\n",
    "To install `missingno` run `!pip install missingno` in a cell. You only ever need to do this once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover bad data:\n",
    "# Option 1: data.isnull().sum()\n",
    "# Option 2: data.isnull().mean()\n",
    "\n",
    "import missingno as msgn\n",
    "msgn.matrix(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values\n",
    "\n",
    "clean_data = data.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msgn.matrix(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features from target\n",
    "y = clean_data['Class']\n",
    "X = clean_data.drop('Class', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Training a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=1987)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X_train,y_train)\n",
    "\n",
    "lr.score(X_test,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc.fit(X_train,y_train)\n",
    "svc.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Model Evaluation\n",
    "\n",
    "We will discuss the evaluation of classification models in detail either at the end of this class or during the next lesson. For now, our main concern is accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reminding ourselves of the baseline we are trying to beat\n",
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the model accuracy\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is very accurate, but is it confident in its predictions? Could we trust it to automate the diagnosis of some patients?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = lr.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(y_proba[:,1], kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(y_proba[:,1], bins=100, kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ideally we'd also spend some time looking at the confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Reading the data\n",
    "df = pd.read_csv('./data/cancer_uci.csv', index_col='Unnamed: 0')\n",
    "\n",
    "# Dropping an unusable column\n",
    "data = df.drop(['Sample_code_number'], axis=1)\n",
    "\n",
    "# Identifying missing numbers\n",
    "data['Bare_Nuclei'] = data['Bare_Nuclei'].apply(lambda x: np.nan if x =='?' else float(x))\n",
    "\n",
    "# Dropping the few rows with missing numbers\n",
    "clean_data = data.dropna(how='any')\n",
    "\n",
    "# Separating the target from the features\n",
    "y = clean_data['Class']\n",
    "X = clean_data.drop('Class', axis=1)\n",
    "\n",
    "# Computing the baseline\n",
    "print('Baseline:', y.value_counts(normalize=True)[0].round(3))\n",
    "\n",
    "# Instantiating a Logistic Regression Model\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Instantiating a Support Vector Machine Model\n",
    "svc = SVC()\n",
    "\n",
    "# Fitting the models\n",
    "lr.fit(X_train, y_train)\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Scoring the models:\n",
    "print('Logistic Regression:', lr.score(X_test, y_test).round(3))\n",
    "print('Support Vector Machine:', svc.score(X_test, y_test).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BONUS WORK 1\n",
    "\n",
    "Try using other `kernels` and `C` values in the SVM model to obtain a higher accuracy on your test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BONUS WORK 2\n",
    "\n",
    "\n",
    "![](https://www.researchgate.net/profile/Juan_Banda/publication/256418526/figure/fig1/AS:297921313558528@1448041384565/Confusion-matrix-example.png)\n",
    "\n",
    "# Confusion Matrix\n",
    "This is a good (if difficult) way to look at your model and where it makes mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I'm training a deliberately good and bad model\n",
    "\n",
    "svc_awesome = SVC(kernel='rbf')\n",
    "svc_good = SVC(kernel='rbf', C=0.01)\n",
    "svc_bad = SVC(kernel='sigmoid')\n",
    "\n",
    "svc_awesome.fit(X_train, y_train)\n",
    "svc_good.fit(X_train, y_train)\n",
    "svc_bad.fit(X_train, y_train)\n",
    "\n",
    "print(svc_awesome.score(X_test, y_test))\n",
    "print(svc_good.score(X_test, y_test))\n",
    "print(svc_bad.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_test, svc_awesome.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.researchgate.net/profile/Juan_Banda/publication/256418526/figure/fig1/AS:297921313558528@1448041384565/Confusion-matrix-example.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_confusion_matrix(y_true, X, model):\n",
    "    classes = model.classes_\n",
    "    confusion_matrix_values = confusion_matrix(y_true, model.predict(X), labels=classes)\n",
    "    data_frame = pd.DataFrame(confusion_matrix_values, columns=classes, index=classes)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_confusion_matrix(y_test,X_test, svc_awesome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or... my way\n",
    "\n",
    "My motto is \n",
    "\n",
    ">**Data Scientists are curious but inherently lazy.**\n",
    "\n",
    "If you find confusion matrices... well confusing then there's probably a better way. The number of TP/FP/TN/FN are important, but the ratios and metrics derived from them are more important. Let's resort to just using those\n",
    "\n",
    "[Documentation in Sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the classification_report package in sklearn.metrics gives the results we want.\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, svc_awesome.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the confusion matrix for the good model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the confusion matrix for the bad model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Food for thought\n",
    "\n",
    "In the case of a model that predicts if a tumor is benign or malignant, which of the following should you pay the most attention to?\n",
    "\n",
    "1. True Positive Rate\n",
    "2. True Negative Rate\n",
    "3. False Positive Rate\n",
    "4. False Negative Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
