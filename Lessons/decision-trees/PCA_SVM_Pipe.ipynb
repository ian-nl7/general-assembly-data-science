{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) PCA, SVM, Pipeline\n",
    "_Bryce Peake (DC) and Steven Longstreet (DC)_\n",
    "\n",
    "This notebook will wrap up with a few advanced machine learning algorithms, as well as using a pipe to automate running models to find the best predictive capacities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries for machine learning\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from matplotlib import pyplot as plt\n",
    "import webbrowser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the iris data from sklearn\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(iris.data, columns = iris.feature_names)\n",
    "df[\"classification\"] = pd.Series(iris.target)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train-test split\n",
    "X = df[[\"sepal length (cm)\", \"sepal width (cm)\", \"petal length (cm)\"]]\n",
    "y = df.classification\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, \n",
    "                                                   random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA\n",
    "PCA as dimensionality reduction\n",
    "\n",
    "Using PCA for dimensionality reduction involves zeroing out one or more of the smallest principal components, resulting in a lower-dimensional projection of the data that preserves the maximal data variance.\n",
    "\n",
    "This reduced-dimension dataset is in some senses \"good enough\" to encode the most important relationships between the points: despite reducing the dimension of the data by 50%, the overall relationship between the data points are mostly preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCALE\n",
    "#scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#fit on training ONLY\n",
    "scaler.fit(X_train)\n",
    "\n",
    "#transform both training and test, based on training fit\n",
    "X_train_transformed = scaler.transform(X_train)\n",
    "X_test_transformed = scaler.transform(X_test)\n",
    "\n",
    "#PCA\n",
    "#import\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#instantiate = n_components keeps the top n features\n",
    "pca = PCA(n_components = 2)\n",
    "\n",
    "#fit\n",
    "pca.fit(X_train)\n",
    "\n",
    "#transform\n",
    "X_train_pca = pca.transform(X_train_transformed)\n",
    "X_test_pca = pca.transform(X_train_transformed)\n",
    "\n",
    "#what happened??\n",
    "print(\"original shape: \", X_train.shape)\n",
    "print(\"transformed shape: \", X_train_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine Classifier\n",
    "\n",
    "\n",
    "- [Support Vector Machine](https://en.wikipedia.org/wiki/Support_vector_machine)\n",
    "\n",
    "**Background** The goal of a support vector machine is to find  the optimal separating hyperplane which maximizes the margin of the training data. \n",
    "\n",
    "**Pros**:\n",
    "    - Powerful Model\n",
    "    - Popularized modern machine learning due to it's extreme power (dethroned by Deep Learning)\n",
    "    - Robust to outliers\n",
    "    - Uses the kernel trick \n",
    "      \n",
    "**Cons**:\n",
    "    - Many possible settings\n",
    "    - Slow to train\n",
    "    - Scale matters\n",
    "    - Can be a black box (it's hard to understand how or why it makes predictions)\n",
    "    - Does not provide predicted probabilities\n",
    "    \n",
    "**What do we know?**\n",
    "\n",
    "- It needs training data so it's a supervised model\n",
    "- We're using it for classification\n",
    "\n",
    "**What do we not know**\n",
    "- How does it make predictions?\n",
    "- What is a hyperplane?\n",
    "\n",
    "Those questions are inherently linked. A **hyperplane** is the seperation of space between our classes. If we break it down further we can better understand it.\n",
    "\n",
    "- in one dimension, an hyperplane is called a point\n",
    "- in two dimensions, it is a line\n",
    "- in three dimensions, it is a plane\n",
    "- in more dimensions you can call it an hyperplane\n",
    "\n",
    "As we start to understand that concept it begs another question. I can draw alot of lines so which is the right one? That's the goal of the SVM - **Finding the optimal hyperplane**. Finding this optimal hyperplane is dependent on a few particular vectors that support its placement - or support vectors.\n",
    "\n",
    "Support Vectors are the data points closest to the hyperplane or decision line. These are the data points that are the **most difficult** to classify. Given their proximity to the hyperplane they have direct bearing on its optimum location. Essentially Support vectors would change the elements of the training set if moved or removed and are critical elements of the training set.\n",
    "\n",
    "![image.png](./assets/Support Vector.png)\n",
    "\n",
    "\n",
    "**Now to optimize our hyperlane!**\n",
    "\n",
    "Step 1 - Seperate the plane as far as you can from data\n",
    "![Optimal Hyperplane](./assets/optimal-hyperplane.png)\n",
    "\n",
    "Step 2 - Find the hyperplane with the largest margin\n",
    "\n",
    "For any hyperplane we can compute the margin.\n",
    " - Find the distance between the hyperplane and the closest data point. \n",
    " - Take that distance and double it\n",
    " \n",
    "Now you have the **margin**. Basically the margin is an area where you will not find any data points. (Note: this can cause some problems when data is noisy)\n",
    "\n",
    "![Margin](./assets/margin.png)\n",
    "\n",
    "\n",
    "\n",
    "### Kernel Trick: For when our data isn't already linearly separable\n",
    "\n",
    "The below picture shows the true magic behind a Support Vector Machine. The objects on the left are mapped as we'd originally find them. A full seperation would require a curve and thus more complexity than drawing a line. In a support vector machine we rearrange using a set of mathmatical functions known as **kernels**. An intuitive way to think about kernels is a similarity function. Given two objects the kernel outputs some similarity score. The simpliest example is the linear kernel or dot-product. Given two vectors, the similarity is the lenght of hte projections of one vector to another. Given a data point to classify, the decision function makes use of the kernel by comparing that data point to a number of support vectors weighted by the learned parameters. The support vectors are in the domain of that data point and along the learned parameters are found by the learning algorithm. \n",
    "\n",
    "![Kernels](./assets/Input_Feature.gif)\n",
    "\n",
    "#### Parameters\n",
    "\n",
    ">class sklearn.svm.SVC(C=1.0, kernel=’rbf’, degree=3, gamma=’auto’, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=’ovr’, random_state=None)\n",
    "\n",
    "Today we're going to adjust C. The C parameter trades off misclassification of training examples against simplicity of the decision surface. A low C makes the decision surface smooth, while a high C aims at classifying all training examples correctly\n",
    "by giving the model freedom to select more samples as support vectors. \n",
    "\n",
    "![BigC_LittleC](./assets/BigC_LittleC.png)\n",
    "\n",
    "As we regularize, or penalize C, we can visualize the impact on our predictions\n",
    "![Regularize](./assets/c_regulation.png)\n",
    "\n",
    "Note: \n",
    "* Here's a good source on [understanding the math](https://www.svm-tutorial.com/2014/11/svm-understanding-math-part-1/) or [The Idiot's Guide to SVM](http://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf)\n",
    "* Learn more about [Kernels](https://en.wikipedia.org/wiki/Kernel_method#Mathematics:_the_kernel_trick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#instantiate\n",
    "svc = SVC()\n",
    "\n",
    "#fit\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "#predict\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "#score\n",
    "print(svc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression, SVM, DTree Pipeline\n",
    "\n",
    "#import\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe_lr = Pipeline([(\"StSclr\", StandardScaler()),\n",
    "                   (\"pca\", PCA(n_components = 2)),\n",
    "                   (\"clf\", LogisticRegression(random_state = 42))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM Pipeline\n",
    "pipe_svm = Pipeline([(\"scl\", StandardScaler()),\n",
    "                    (\"pca\", PCA(n_components = 2)),\n",
    "                    (\"clf\", svm.SVC(random_state = 42))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DT Pipeline\n",
    "pipe_dt = Pipeline([(\"sc\", StandardScaler()),\n",
    "                    (\"pca\", PCA(n_components = 2)),\n",
    "                    (\"clf\", tree.DecisionTreeClassifier(random_state = 42))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the pipeline objects\n",
    "pipelines = [pipe_lr, pipe_svm, pipe_dt]\n",
    "pipe_dict = {0: \"Logistic Classifier\", 1: \"Support Vector Machine\", \n",
    "             2: \"Decision Tree\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the pipes through automation\n",
    "for pipe in pipelines: \n",
    "    pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare accuracies\n",
    "for idx, val in enumerate(pipelines):\n",
    "    print(\"%s pipeline test accuracy: %.3f\" % (pipe_dict[idx], val.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0.0\n",
    "best_clf = 0\n",
    "best_pipe = ''\n",
    "for idx, val in enumerate(pipelines):\n",
    "    if val.score(X_test, y_test) > best_acc:\n",
    "        best_acc = val.score(X_test, y_test)\n",
    "        best_pipe = val\n",
    "        best_clf = idx\n",
    "print('Classifier with best accuracy: %s' % pipe_dict[best_clf])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together - DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([(\"scl\", StandardScaler()),\n",
    "                (\"pca\", PCA(n_components = 2)),\n",
    "                (\"clf\", tree.DecisionTreeClassifier(random_state = 42))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set param range\n",
    "param_range = list(range(1, 15, 1))\n",
    "grid_params = [{'clf__criterion': ['gini', 'entropy'],\n",
    "        'clf__min_samples_leaf': param_range,\n",
    "        'clf__max_depth': param_range,\n",
    "        'clf__min_samples_split': param_range[1:],\n",
    "        'clf__presort': [True, False]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "gs = GridSearchCV(estimator = pipe, \n",
    "                 param_grid = grid_params,\n",
    "                 scoring = \"accuracy\",\n",
    "                 cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "url = \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\"\n",
    "webbrowser.open(url,new=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best params:\", gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your turn\n",
    "For this part of the lab, do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the cancer dataset from sklearn, and structure as a pandas dataframe. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the best possible model using pipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now, put the pipe into a gridsearch\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "0.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
