{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis\n",
    "\n",
    "PCA isn't exactly full machine learning algorithm, but instead an unsupervised learning algorithm. It is often used to **preprocess** data before it goes into a supervised learning method. Traditionally it is used to solve problems involving too many features and multicolinearity. \n",
    "\n",
    "## Let's dig into how it works \n",
    "Suppose you have $p$ feature columns. The **first principal component** is a linear combination of all $p$ columns that accounts for the **maximum variance** among them.  That is,\n",
    "\n",
    "$$ z_1 = c_{11}x_1 + c_{12}x_2 + c_{13}x_3 + ... c_{1n}$$\n",
    "\n",
    "The **second principal component** is another linear combination of the $p$ features that accounts for the maximum of the _remaining_ variance after the first. Another condition is that the second PC must be **orthogonal (perpendicular)** to the first.\n",
    "\n",
    "$$ z_2 = c_{21}x_1 + c_{22}x_2 + c_{23}x_3 + ... c_{2n}$$\n",
    "\n",
    "The **third principal component** maximizes the remaining variance while being orthogonal (read: _independent_) to the first two, and so on.\n",
    "\n",
    "\n",
    "$$ z_3 = c_{31}x_1 + c_{32}x_2 + c_{33}x_3 + ... c_{3n}$$\n",
    "$$...$$\n",
    "$$ z_i = c_{i1}x_1 + c_{i2}x_2 + c_{i3}x_3 + ... c_{in}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MachineLearning](assets/PCA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hands on visuals are a great way to learn. Here is an interactive website to help you understand how [PCA works](http://setosa.io/ev/principal-component-analysis/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "Let's work with the cancer data set again since it had so many features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cancer['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(cancer['data'],columns=cancer['feature_names'])\n",
    "#(['DESCR', 'data', 'feature_names', 'target_names', 'target'])\n",
    "ydf = pd.DataFrame(cancer['target'],columns=['malignant'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=cancer.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat([X,ydf],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Visualization\n",
    "\n",
    "As we've noticed before it is difficult to visualize high dimensional data, we can use PCA to find the first two principal components, and visualize the data in this new, two-dimensional space, with a single scatter-plot. Before we do this though, we'll need to scale our data so that each feature has a single unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA with Scikit Learn uses a very similar process to other preprocessing functions that come with SciKit Learn. We instantiate a PCA object, find the principal components using the fit method, then apply the rotation and dimensionality reduction by calling transform().\n",
    "\n",
    "We can also specify how many components we want to keep when creating the PCA object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can transform this data to its first 2 principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pca = pca.transform(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_import=pd.DataFrame(pca.components_,columns=cancer['feature_names'],index = ['PC-1','PC-2'])\n",
    "feat_import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We've reduced 30 dimensions to just 2! Let's plot these two dimensions out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(x_pca[:,0],x_pca[:,1],c=cancer['target'],cmap='plasma')\n",
    "plt.xlabel('First principal component')\n",
    "plt.ylabel('Second Principal Component')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly by using these two components we can easily separate these two classes.\n",
    "\n",
    "## Interpreting the components \n",
    "\n",
    "Unfortunately, with this great power of dimensionality reduction, comes the cost of being able to easily understand what these components represent.\n",
    "\n",
    "The components correspond to combinations of the original features, the components themselves are stored as an attribute of the fitted PCA object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(pca.components_[0],df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this numpy matrix array, each row represents a principal component, and each column relates back to the original features. Fir example, here is the first component:\n",
    "\n",
    "$$z_1 = 0.21890244x_1 + 0.10372458x_2 + 0.22753729x_3 + 0.22099499x_4 + 0.14258969x_5 + ... + 0.13178394x_{30}$$\n",
    "\n",
    "we can visualize this relationship with a heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comp = pd.DataFrame(pca.components_,columns=cancer['feature_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.heatmap(df_comp,cmap='plasma',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This heatmap shows the how each variable contributes to each of our two principle components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's the right number of n_components?\n",
    "\n",
    "Finding the right number with PCA can be accomplished by applying it to a model and seeing where it best maximizing it's score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does it look with the whole feature set?\n",
    "\n",
    "lg=LogisticRegression()\n",
    "lg.fit(X,y)\n",
    "lg.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How about reducing it to our two pca features?\n",
    "lg=LogisticRegression()\n",
    "lg.fit(x_pca,y)\n",
    "lg.score(x_pca,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does this tell us?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing Pipeline\n",
    "\n",
    "To find the right value of n_components we could cycle through this a few times. However - to save us some time lets introduce the sklearn package - [Pipeline](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n",
    "\n",
    "Pipeline sequentially applies a list of transforms and passes them to a final estimator. Intermediate steps of the pipeline must be ‘transforms’, that is, they must implement fit and transform methods. The final estimator only needs to implement fit. The transformers in the pipeline can be cached using memory argument.\n",
    "\n",
    "Included **Methods**\n",
    "\n",
    "| Method                     | Application   |\n",
    "|:---------------------------|------------:|\n",
    "|decision_function(X)        |\tApply transforms, and decision_function of the final estimator|\n",
    "|fit(X[, y])                 |\tFit the model|\n",
    "|fit_predict(X[, y])         |\tApplies fit_predict of last step in pipeline after transforms.|\n",
    "|fit_transform(X[, y])       |\tFit the model and transform with the final estimator|\n",
    "|get_params([deep])          |\tGet parameters for this estimator.|\n",
    "|predict(X)                  |\tApply transforms to the data, and predict with the final estimator|\n",
    "|predict_log_proba(X)        |\tApply transforms, and predict_log_proba of the final estimator|\n",
    "|predict_proba(X)            |\tApply transforms, and predict_proba of the final estimator|\n",
    "|score(X[, y, sample_weight])|\tApply transforms, and score with the final estimator|\n",
    "|set_params(**kwargs)        |\tSet the parameters of this estimator.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizing for variance of n_components could take awhile. \n",
    "# Let's make our coding easier by putting all these models into a pipeline\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipe = Pipeline([\n",
    "    ('sc', StandardScaler()),\n",
    "    ('pc', PCA(n_components=2)),\n",
    "    ('lg', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What will the Standard Scalar do to our data before we fit each model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X,y)\n",
    "pipe.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What's our model look like?\n",
    "pipe.get_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can use the train/test split procedure from the previous lesson to see how each of these 30 versions performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This for loop goes through every number of possible components and tests the accuracy of a model fitted to that many components.\n",
    "\n",
    "acc_list = []\n",
    "k_range = range(1,X.shape[1] + 1)\n",
    "for k in k_range:\n",
    "    pipe.set_params(pc__n_components=k)\n",
    "    pipe.fit(X_train, y_train)\n",
    "    acc = pipe.score(X_test, y_test)\n",
    "    acc_list.append(acc)\n",
    "    print(f\"k = {k}: Acc = {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(acc_list);\n",
    "plt.ylabel('Accuracy - higher is better')\n",
    "plt.xlabel('Number of Components Included');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which number of n_components would you choose? why?\n",
    "\n",
    "#### Hint: what were we trying to reduce with PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can visualize the list\n",
    "k_list=dict(zip(k_range,acc_list))\n",
    "k_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also find the greatest variance explained (for the first time)\n",
    "\n",
    "print(sorted(k_list.items(), key=lambda x: (x[1]), reverse=True)[0])\n",
    "print(sorted(k_list.items(), key=lambda x: (x[1]), reverse=True)[0][0])\n",
    "best_k=sorted(k_list.items(), key=lambda x: (x[1]), reverse=True)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What happens when we add that back into our pipe?\n",
    "pipe_best_k = Pipeline([\n",
    "    ('sc', StandardScaler()),\n",
    "    ('pc', PCA(n_components=[Insert the value you want])),\n",
    "    ('lg', LogisticRegression())\n",
    "])\n",
    "pipe_best_k.fit(X,y)\n",
    "pipe_best_k.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instead of setting n_components\n",
    "\n",
    "Another approach to PCA is setting it against the amount of variance you want to explain\n",
    "\n",
    "**Note - need a new dataset for this one!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The first approch is to see how many component explain variance. After the first 4... not so much\n",
    "pca_graph=PCA().fit(cancer.data)\n",
    "plt.plot(np.cumsum(pca_graph.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set a model that'll explain 95% of variance\n",
    "pca=PCA(.95)\n",
    "pca.fit(X_train)\n",
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: Why did our model only include one component?  Would the model be more or less accurate if more components were included?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary:\n",
    "\n",
    "### What does it do?\n",
    "* Creates *linearly independent* predictors\n",
    "* Allows you to only use the most valuable features\n",
    "\n",
    "### What are the components?\n",
    "* Values calculated from the raw observations:\n",
    "\n",
    "$$ z_1 = c_{11}x_1 + c_{12}x_2 + c_{13}x_3 + ... c_{1n}$$\n",
    "$$ z_2 = c_{21}x_1 + c_{22}x_2 + c_{23}x_3 + ... c_{2n}$$\n",
    "$$...$$\n",
    "$$ z_i = c_{i1}x_1 + c_{i2}x_2 + c_{i3}x_3 + ... c_{in}$$\n",
    "\n",
    "* $z_1$ is always the strongest predictor. \n",
    "* $z_2$ is always the strongest predictor that is completely independent from $z_1$.  \n",
    "* $z_3$ is always the strongest predictor that is completely independent from $z_1$ and $z_2$.  \n",
    "* We can keep this going until the number of compoenents equals the number of predictors\n",
    "\n",
    "### Why do we exclude components from our model?\n",
    "* If we included all components, we would be getting the exact same result as if we hadn't used PCA\n",
    "* By excluding components we can avoid overfitting the model, we are essentially ignoring the information that is least reliable.\n",
    "\n",
    "### Points to Remember\n",
    "* PCA is used to overcome features redundancy in a data set.\n",
    "* These features are low dimensional in nature.\n",
    "* These features a.k.a components are a resultant of normalized linear combination of original predictor variables.\n",
    "* These components aim to capture as much information as possible with high explained variance.\n",
    "* The first component has the highest variance followed by second, third and so on.\n",
    "* The components must be uncorrelated (remember orthogonal direction ? ). See above.\n",
    "* Normalizing data becomes extremely important when the predictors are measured in different units.\n",
    "* PCA works best on data set having 3 or higher dimensions. Because, with higher dimensions, it becomes increasingly difficult to make interpretations from the resultant cloud of data.\n",
    "* PCA is applied on a data set with numeric variables.\n",
    "* PCA is a tool which helps to produce better visualizations of high dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
